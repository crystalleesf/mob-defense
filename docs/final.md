---
layout: default
title: Final Report
---

## Video Summary


## Project Summary
<img src="./images/mob-defense-screenshot-2.png" alt="Mob Defense Screenshot" width="500"/>

The goal of Mob Defense is to train the agent to efficiently and effectively kill Zombies. We've placed the agent in a 20x20 arena with 4 Zombies and 50 sheep. The agent is equipped with an enchanted diamond sword and night vision. We initially considered respawning Zombies after the agent kills a Zombie, however we felt that this may convolute our data. We chose to only spawn 4 Zombies per episode to maintain a consistent learning environment, which will ensure that the agent's ability to attack and survive is actually improving. We also placed 50 sheep in the arena as obstacles for the agent. As sheep are friendly mobs in Minecraft, we wanted the agent to learn to avoid attacking and killing the sheep. Each episode spans two minutes and concludes if time runs out or the agent gets killed by the Zombies. Its mission is to kill all 4 Zombies before time runs out while incurring minimal damage to health.  

This task requires AI/ML techniques to achieve because in order for the agent to survive it needs to make strategic decisions on whether it should run away from the Zombies (fight or flight), which Zombie to attack first, and the unintended damage it may cause to surrounding sheep. For example, if the agent exists in a state where a Zombie is surrounded by a lot of sheep, it may be more ideal for the agent to move away and wait for the sheep to disperse, instead of attacking the Zombie at that moment and possibly racking up negative reward for unintentionally damaging the surrounding sheep. 

## Approaches
Originally, we had the agent move and turn towards the "Zombiest" point. The "Zombiest" point was calculated using the health of the Zombies and the area with the highest number of Zombies. This method prioritized Zombies with lower health because they are easier for the agent to kill. Then, the agent would consider the area with a high number of Zombies because that is where there is greatest opportunity for positive reward. However, we observed that  if the agent tries to attack and turn away from a Zombie in the "Zombiest" point, it was unable to turn away fast enough from the other Zombies and would continue getting hit and die.  

In this final sprint, we are using reinforcement learning to train our agent to attack Zombies, while keeping itself and the surrounding sheep alive. Our reinforcement learning model is based on a -ecause after every step in an episode, the agent must make the best decision based on the current state generated by the its previous action. After each step, the environment's state changes because the agent's health may have decreased, the Zombies' health may have decreased, and the position of the Zombies may have changed. In each state, we've created a continuous action space based off of Assignment 2. At each step, the agent has the ability to `move [0...1]`, `turn [0...1]`, `attack 0`, and `attack 1`. When a Zombie comes into the agent's "line of sight", the agent will attack the Zombie using its enchanted diamond sword. Additionally, we've programmed the agent to stop moving and turning when it sees a Zombie, so that it has enough time to attack the Zombie to its fullest ability.  

<img src="./images/mdp-final.png" alt="Markov Decision Process (Final)" width="500"/>  

The reward structure is as follows: 
| <div style="width:290px">Reward</div>  | Action |
| ------------- | ------------- |
| +2  | Agent damages a Zombie (exact float value can vary between [0...2] on the damage severity )  |
| -0.5  | Agent damages a Sheep  |
| -1  | Agent's health decreases due to being attacked by a Zombie  |
| +(4 - num_zombie_remaining) * 5  | After each episode, the agent can receive a positive reward based on the number of Zombies it has killed. (Ex: If only 2 Zombies remained: (4-2)*5 = 10)  |
| -(50 - num_sheep_remaining) * 0.5  | After each episode, the agent can receive a negative reward based on the number of Sheep it has killed. (Ex: If only 2 Zombies remained: -(50-48)*0.5 = -1)  |


## Evaluation


## References
- [Malmo XML Schema Documentation](https://microsoft.github.io/malmo/0.21.0/Schemas/MissionHandlers.html)
- [Minecraft Enchantment Command](https://www.digminecraft.com/game_commands/enchant_command.php)
- `hit_test.py` from Malmo's Python Examples
- UC Irvine CS 175's Assignment 2
- [GeeksforGeeks MDP Explanation](https://www.geeksforgeeks.org/markov-decision-process/)
- [MDP Diagram](https://www.google.com/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-reinforcement-learning-markov-decision-process-44c533ebf8da&psig=AOvVaw0fLUNNQBsAcZyybBKKqMg1&ust=1639008147546000&source=images&cd=vfe&ved=0CAwQjhxqFwoTCKCJp6vz0vQCFQAAAAAdAAAAABAD)